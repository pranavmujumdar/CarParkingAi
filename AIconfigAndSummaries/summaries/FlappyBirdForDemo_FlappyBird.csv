Steps,Policy/Entropy,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate
60000,0.42584726,10.209431021058364,21.670303975058456,1.0998602,10.209431019989848,20.550247,0.04958478,0.0002990746
120000,0.18918788,105.25806451399266,209.01433691756273,6.3235745,105.25806447382897,30.71711,0.048023183,0.00029727846
180000,0.11734052,79.02271540373486,157.4046997389034,10.291878,79.02271539159605,26.650042,0.053028982,0.00029548025
240000,0.13829291,155.90736841873118,308.80526315789473,14.081254,155.90736840555542,23.86592,0.049882602,0.00029368215
300000,0.13796845,186.69937106835766,369.34591194968556,18.491922,186.69937113283567,21.650738,0.046890296,0.00029188182
360000,0.1536664,193.65131578829727,383.05263157894734,22.759874,193.6513158169232,21.145664,0.048521154,0.00029008105
