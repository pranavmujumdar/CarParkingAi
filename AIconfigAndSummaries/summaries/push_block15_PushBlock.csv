Steps,Policy/Entropy,Environment/Cumulative Reward,Environment/Episode Length,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Losses/Value Loss,Losses/Policy Loss,Policy/Learning Rate
60000,1.2486656,0.9861323429269697,13.194246314788398,0.8561694,0.9861300934863573,0.014118038,0.06886532,0.00029533904
120000,0.2470291,0.9932603584654301,6.1428401380788005,0.9655721,0.9932609555375688,0.00015494214,0.06581593,0.00028639106
180000,0.20193647,0.9931303209720584,6.267870123576448,0.96691346,0.993130007930694,0.00011751557,0.06949368,0.00027744382
240000,0.168768,0.993148975173899,6.249124078772502,0.96469057,0.99314919440779,7.760203e-05,0.06785458,0.00026850036
300000,0.16358614,0.9933345886388681,6.059654076950229,0.9660169,0.9933336649794078,0.000105308856,0.0656706,0.00025956242
360000,0.12758827,0.9935067789027955,5.897459478100931,0.96569324,0.9935077575842539,6.6708104e-05,0.066899136,0.00025062286
420000,0.12322981,0.9936840820064217,5.714493564633464,0.96772975,0.9936837714088317,7.819775e-05,0.06713025,0.00024152873
